---
title: "Capstone Final Report"
author: "Nick Glass"
date: "12/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview:

The original goal of this project was to predict the amount of goals ECHL players create while they are on the ice. The idea was to capture their contribution by different aspects of the game such as 5 on 5 offense, 5 on 5 defense, power play, and penalty kill. The models would be broken up by forwards and defense, with the data providing game by game analysis for coaches to make decisions on player usage. 

After complications with acquiring the data, time limitations, and not establishing a clear method to work with the coaching staff I decided to work with NHL seasonal data due to it being publicly available and easy to access. Furthermore, after realizing the amount of time this project would take and the fact that this would be my first major project I decided to only focus on  5 on 5 offense and 5 on 5 defense broken down by forwards and defensemen since this is the most common state of the game. Also, I decided to focus on only regression models to predict the amount of goals the players will score the next season not the amount of goals the player will create. For the models that predict how the player will prevent goals I decided to predict how many goals are scored against the players team when that player is on the ice. After making these changes my new goal for the project was to become more familiar with hockey analytics models and regression methods so I can build more complex models in the future. 

```{r, warning=FALSE, include=FALSE}
# Load packages
library(readr)
library(tidyverse)
library("GGally")
library(HH)
library(leaps)
library(forecast)
library(MASS)
```



# Load the data:

```{r, warning=FALSE}
# Load NHL individual stats 2013-14
## 2013-14 ##
IND_5V5_13_14 <- read_csv("Player_Season_Totals_13_14_IND_5V5.csv",
                          col_types = "cccciniiiiininniiiiiiiiiiiiiiiiiiin")
head(IND_5V5_13_14)

# Delete unnecessary columns
IND_5V5_13_14 <- IND_5V5_13_14[,-1]

# Load NHL on-ice stats 2013-14
OI_5V5_13_14 <- read_csv("Player_Season_Totals_13_14_OI_5V5.csv",
                         col_types = "cccciniiniiniiniinnnniiniiniiniiniiniiniinnnniiiiniiin")
head(OI_5V5_13_14)

# Delete unnecessary columns
OI_5V5_13_14 <- OI_5V5_13_14[, -c(1,3,4,5,6)]

# Join the two tables for 2013-14
Data_13_14 <- merge(x=IND_5V5_13_14,y=OI_5V5_13_14,by="Player",all=TRUE)
head(Data_13_14)

## 2014-15 ##
# Load NHL individual stats 2014-15
IND_5V5_14_15 <- read_csv("Player_Season_Totals_14_15_IND_5V5.csv",
                          col_types = "cccciniiiiininniiiiiiiiiiiiiiiiiiin")

# Delete unnecessary columns
IND_5V5_14_15 <- IND_5V5_14_15[,-1]

# Load NHL on-ice stats 2014-15
OI_5V5_14_15 <- read_csv("Player_Season_Totals_14_15_OI_5V5.csv",
                         col_types = "cccciniiniiniiniinnnniiniiniiniiniiniiniinnnniiiiniiin")

# Delete unnecessary columns
OI_5V5_14_15 <- OI_5V5_14_15[, -c(1,3,4,5,6)]

# Join the two tables for 2014-15
Data_14_15 <- merge(x=IND_5V5_14_15,y=OI_5V5_14_15,by="Player",all=TRUE)

## 2015-16 ##
# Load NHL individual stats 2015-16
IND_5V5_15_16 <- read_csv("Player_Season_Totals_15_16_IND_5V5.csv",
                          col_types = "cccciniiiiininniiiiiiiiiiiiiiiiiiin")

# Delete unnecessary columns
IND_5V5_15_16 <- IND_5V5_15_16[,-1]

# Load NHL on-ice stats 2015-16
OI_5V5_15_16 <- read_csv("Player_Season_Totals_15_16_OI_5V5.csv",
                         col_types = "cccciniiniiniiniinnnniiniiniiniiniiniiniinnnniiiiniiin")

# Delete unnecessary columns
OI_5V5_15_16 <- OI_5V5_15_16[, -c(1,3,4,5,6)]

# Join the two tables for 2015-16
Data_15_16 <- merge(x=IND_5V5_15_16,y=OI_5V5_15_16,by="Player",all=TRUE)

## 2016-17 ##
# Load NHL individual stats for 2016-17
IND_5V5_16_17 <- read_csv("Player_Season_Totals_16_17_IND_5V5.csv",
                          col_types = "cccciniiiiininniiiiiiiiiiiiiiiiiiin")

# Delete unnecessary columns
IND_5V5_16_17 <- IND_5V5_16_17[,-1]

# Load NHL on-ice stats 2016-17
OI_5V5_16_17 <- read_csv("Player_Season_Totals_16_17_OI_5V5.csv",
                         col_types = "cccciniiniiniiniinnnniiniiniiniiniiniiniinnnniiiiniiin")

# Delete unnecessary columns
OI_5V5_16_17 <- OI_5V5_16_17[, -c(1,3,4,5,6)]

# Join the two tables for 2016-17
Data_16_17 <- merge(x=IND_5V5_16_17,y=OI_5V5_16_17,by="Player",all=TRUE)

## 2017-18 ##
# Load NHL individual stats 2017-18
IND_5V5_17_18 <- read_csv("Player_Season_Totals_17_18_IND_5V5.csv",
                          col_types = "cccciniiiiininniiiiiiiiiiiiiiiiiiin")

# Delete unnecessary columns
IND_5V5_17_18 <- IND_5V5_17_18[,-1]

# Load NHL on-ice stats 2017-18
OI_5V5_17_18 <- read_csv("Player_Season_Totals_17_18_OI_5V5.csv",
                         col_types = "cccciniiniiniiniinnnniiniiniiniiniiniiniinnnniiiiniiin")

# Delete unnecessary columns
OI_5V5_17_18 <- OI_5V5_17_18[, -c(1,3,4,5,6)]

# Join the two tables for 2017-18
Data_17_18 <- merge(x=IND_5V5_17_18,y=OI_5V5_17_18,by="Player",all=TRUE)

## 2018-19 ##
# Load NHL individual stats 2018-19
IND_5V5_18_19 <- read_csv("Player_Season_Totals_18_19_IND_5V5.csv",
                          col_types = "cccciniiiiininniiiiiiiiiiiiiiiiiiin")

# Delete unnecessary columns
IND_5V5_18_19 <- IND_5V5_18_19[,-1]

# Load NHL on-ice stats 2018-19
OI_5V5_18_19 <- read_csv("Player_Season_Totals_18_19_OI_5V5.csv",
                         col_types = "cccciniiniiniiniinnnniiniiniiniiniiniiniinnnniiiiniiin")

# Delete unnecessary columns
OI_5V5_18_19 <- OI_5V5_18_19[, -c(1,3,4,5,6)]

# Join the two tables for 2018-19
Data_18_19 <- merge(x=IND_5V5_18_19,y=OI_5V5_18_19,by="Player",all=TRUE)
```

# Analysis:

The data for this project is from Natural Stat Trick which is a hockey data website. The data selected is of NHL players from 2013-2019. Each year of data was loaded separately with two data files per season. One of the types of data includes individual statistics and the other includes counts of statistics while the player was on the ice. These data sets were merged together so there would be only one set of data per year.


# Create the response variables column:

```{r, warning=FALSE}
# Combine statistics with the next years actual goals 
# Find the goals of 2014_15
goals14_15 <- Data_14_15 %>%
  dplyr::select(Player, Goals)

head(goals14_15)
dim(goals14_15)

# Join goals14_15 with the 2013-14 data
clean_data13_14 <- inner_join(Data_13_14,
                              goals14_15,
                              by="Player")
head(clean_data13_14)

# Find the goals of 2015_16
goals15_16 <- Data_15_16 %>%
  dplyr::select(Player, Goals)

# Join goals15_16 with the 2014-15 data
clean_data14_15 <- inner_join(Data_14_15,
                              goals15_16,
                              by="Player")

# Find the goals of 2016_17
goals16_17 <- Data_16_17 %>%
  dplyr::select(Player, Goals)

# Join goals16_17 with the 2015-16 data
clean_data15_16 <- inner_join(Data_15_16,
                              goals16_17,
                              by="Player")

# Find the goals of 2017_18
goals17_18 <- Data_17_18 %>%
  dplyr::select(Player, Goals)


# Join goals17_18 with the 2016-17 data
clean_data16_17 <- inner_join(Data_16_17,
                              goals17_18,
                              by="Player")

# Find the goals of 2018_19
goals18_19 <- Data_18_19 %>%
  dplyr::select(Player, Goals)

# Join goals18_19 with the 2017-18 data
clean_data17_18 <- inner_join(Data_17_18,
                              goals18_19,
                              by="Player")

# Combine the five seasons worth of data
hockey_data <- rbind(clean_data13_14, clean_data14_15, clean_data15_16,clean_data16_17, clean_data17_18)

head(hockey_data)
```

# Analysis:

For the models that predict a players offense the data was joined with the count of goals the players scored in the following year. This new column was named future goals and is the response variable for the offensive models. Each of the years were then combined to form a single new data frame that contains all of the necessary information called hockey_data. This data set includes 3646 observations or players and 83 columns or variables. 

# Preparing the data for the forward offensive model:

```{r, warning=FALSE}
# Rename the Goals variables
hockey_data <- rename(hockey_data, Goals = Goals.x)
hockey_data <- rename(hockey_data, Future_Goals = Goals.y)
hockey_data <- rename(hockey_data, First_Assists = `First Assists`)
hockey_data <- rename(hockey_data, Second_Assists = `Second Assists`)
hockey_data <- rename(hockey_data, Rush_Attempts = `Rush Attempts`)
hockey_data <- rename(hockey_data, Rebounds_Created = `Rebounds Created`)
hockey_data <- rename(hockey_data, Off_Zone_Starts = `Off. Zone Starts`)
hockey_data <- rename(hockey_data, Neu_Zone_Starts = `Neu. Zone Starts`)
hockey_data <- rename(hockey_data, Def_Zone_Starts = `Def. Zone Starts`)
hockey_data <- rename(hockey_data, On_The_Fly_Starts = `On The Fly Starts`)

# Find the missing values by column
Missing_values <- sort(colSums(is.na(hockey_data)), decreasing = TRUE)
Missing_values <- Missing_values[Missing_values > 0]
print(sort(Missing_values), decreasing = TRUE)

# Prepare the data for the forwards 5v5 offense model
Forward_Offense <- hockey_data %>%
  filter(Position == "R"|Position == "L"|Position == "C") %>%
  dplyr::select(Player:Rebounds_Created, Takeaways, On_The_Fly_Starts,                 Off_Zone_Starts:Def_Zone_Starts, Future_Goals)

# Insert zero for the missing IPP values
Forward_Offense$IPP[is.na(Forward_Offense$IPP)] = 0

# Find the dimensions of the data
dim(Forward_Offense)

# Filtering for games played and teams played
Forward_Offense <- hockey_data %>%
  filter(Position == "R"|Position == "L"|Position == "C", GP > 10) %>%
  filter(str_detect(Team, pattern=",", negate=TRUE)) %>%
  dplyr::select(Player:Rebounds_Created, Takeaways, On_The_Fly_Starts,                 Off_Zone_Starts:Def_Zone_Starts, Future_Goals)

# Find the dimensions of the data after filtering by GP and Team
dim(Forward_Offense)

# Add one goal to the future goals 
Forward_Offense <- Forward_Offense %>%
  mutate(Future_Goals = Future_Goals + 1)

# Drop unwanted columns
Forward_Offense <- Forward_Offense[, -c(7,10,12,13,15)]
head(Forward_Offense)
```

# Analysis:

Before the regression models could be made there needed to be some preprocessing. First, some variables were renamed to make them more convenient to work with. Second, the number of missing variables were identified and dealt with. For IPP which is the individual points percentage of the player, some values were NA when they should have been 0. In this case the value of 0 was imputed for the NA values. It turns out that all of the other variables that had missing values were not used anyway in the analysis. Third, the data set was filtered to only included centers, left wings, and right wings, otherwise known as forwards. The full data set for forwards includes 2291 observations or players and 26 columns or variables. In the subset of the data after filtering out players who played with more than one team in a season or played in less then 10 games, there is 1856 observations and 26 variables. After dropping unwanted variables the data set includes Player, Team, Position, GP, TOI, Goals, First Assists, Second Assists, IPP, ixG, iFF, iSCF, iHDCF, Rush_Attempts, Rebounds_Created, Takeaways, On_The_Fly_Starts, Neu_Zone_Starts, Def_Zone_Starts, and Future_Goals. Finally, one goal was added to the Future goals column in order to run the box cox transformation in the future.  

# Creating the models to predict the forwards offense:

```{r, warning=FALSE}
# Regression analysis 
# Splitting data into training and validation sets
set.seed(1)
train.rows.FO <- sample(rownames(Forward_Offense), nrow(Forward_Offense) * 0.7)
train.data.FO <- Forward_Offense[train.rows.FO, ]
valid.rows.FO <- setdiff(rownames(Forward_Offense), train.rows.FO)
valid.data.FO <- Forward_Offense[valid.rows.FO, ]

# Create a MLR model that contains the full desired predictors 
FOM_Full <- lm(Future_Goals ~ Goals + First_Assists + Second_Assists +
                            + IPP + ixG + iFF + iSCF + iHDCF +       
                            Rush_Attempts + Rebounds_Created + 
                            Takeaways + On_The_Fly_Starts + 
                            Off_Zone_Starts + Neu_Zone_Starts + 
                            Def_Zone_Starts, data=train.data.FO)

# Summary
summary(FOM_Full) 

# Get list of residuals 
Res_FOM_Full <- resid(FOM_Full)

# Produce residual vs. fitted plot
plot(fitted(FOM_Full), Res_FOM_Full)

# Add a horizontal line at 0 
abline(0,0)

# Create Q-Q plot for residuals
qqnorm(Res_FOM_Full)

# Add a straight diagonal line to the plot
qqline(Res_FOM_Full) 

# Create density plot of residuals
plot(density(Res_FOM_Full))

# Calculate the error
accuracy(FOM_Full)

# Out-of-sample Prediction
pred_FOM_Full <- predict(FOM_Full, newdata=valid.data.FO)
pred_FOM_Full

# Find the error of the validation data
accuracy(pred_FOM_Full, valid.data.FO$Future_Goals)

# Applying Box-Cox transformation to lm object
bc.results=boxcox(FOM_Full) #Applying Box-Cox transformation to lm object
lambda=bc.results$x[which.max(bc.results$y)] #extract the best lambda
lambda

lambda_FOM_Full <-lm(sqrt(Future_Goals) ~ Goals + First_Assists +  
                      Second_Assists + IPP + ixG + iFF + iSCF + iHDCF +
                      Rush_Attempts + Rebounds_Created + Takeaways + 
                      On_The_Fly_Starts + Off_Zone_Starts +
                      Neu_Zone_Starts +     
                      Def_Zone_Starts,data=train.data.FO)

# Summary
summary(lambda_FOM_Full) 

# Get list of residuals 
lambda_Res_FOM_Full <- resid(lambda_FOM_Full)

# Produce residual vs. fitted plot
plot(fitted(lambda_FOM_Full), lambda_Res_FOM_Full)

# Add a horizontal line at 0 
abline(0,0)

# Create Q-Q plot for residuals
qqnorm(lambda_Res_FOM_Full)

# Add a straight diagonal line to the plot
qqline(lambda_Res_FOM_Full) 

# Create density plot of residuals
plot(density(lambda_Res_FOM_Full))

# Compute VIF
vif(FOM_Full)

# Compute VIF
vif(lambda_FOM_Full)

# Calculate the error
accuracy(lambda_FOM_Full)

# Out-of-sample Prediction
pred_lambda_FOM_Full <- predict(lambda_FOM_Full, newdata=valid.data.FO)

# Undo the previous transformations
Real_pred_lambda_FOM_Full <- pred_lambda_FOM_Full^2 - 1
Real_pred_lambda_FOM_Full

# Find the error of the validation data
accuracy(Real_pred_lambda_FOM_Full, valid.data.FO$Future_Goals)

# Best subset selection
best_subset_FOM=regsubsets(sqrt(Future_Goals) ~ Goals + First_Assists +
                           Second_Assists + IPP + ixG + iFF + iSCF + 
                            iHDCF + Rush_Attempts + Rebounds_Created +
                            Takeaways + On_The_Fly_Starts + 
                            Off_Zone_Starts + Neu_Zone_Starts +
                          Def_Zone_Starts, data=train.data.FO,nvmax=15)

# Summarize the result
best_summary_FOM=summary(best_subset_FOM)
best_summary_FOM 

# Find the best Cp
p=2:16
Cp=cbind(p,best_summary_FOM$cp) #Pair p and Cp
colnames(Cp)=c('p','Cp') #Define column names 
print(Cp)

# Find BIC
p=2:16
BIC=cbind(p,best_summary_FOM$bic) #Pair p and BIC
colnames(BIC)=c('p','BIC') #Define column names 
print(BIC) 
plot(BIC,cex=2,pch=19) # Plot of BIC

# Find AIC
n=nrow(train.data.FO)
p=BIC[,1]
aic=BIC[,2]-p*log(n)+2*p #Need to compute AIC using BIC
AIC=cbind(2:16,aic) #Pair p and AIC
colnames(AIC)=c('p','AIC') #Define column names 
print(AIC) 
plot(AIC,cex=2,pch=19) #Plot of AIC

# Best Cp Model
FOM_BestCp <- lm(sqrt(Future_Goals) ~ Goals + First_Assists + iSCF + 
                   Takeaways + On_The_Fly_Starts + Off_Zone_Starts + 
                   Neu_Zone_Starts,data=train.data.FO)
# Summary
summary(FOM_BestCp)

# Compute VIF
vif(FOM_BestCp)

# Calculate the error
accuracy(FOM_BestCp)

# Out-of-sample Prediction
pred_FOM_BestCp <- predict(FOM_BestCp, newdata=valid.data.FO)

# Undo the previous transformations
Real_pred_FOM_BestCp <- pred_FOM_BestCp^2 - 1
Real_pred_FOM_BestCp

# Find the error of the validation data
accuracy(Real_pred_FOM_BestCp, valid.data.FO$Future_Goals)

# Best BIC Model
FOM_BestBIC <- lm(sqrt(Future_Goals) ~ Goals + iSCF + Takeaways + 
                    On_The_Fly_Starts + 
                    Off_Zone_Starts,data=train.data.FO)
# Summary
summary(FOM_BestBIC)

# Compute VIF
vif(FOM_BestBIC)

# Calculate the error
accuracy(FOM_BestBIC)

# Out-of-sample Prediction
pred_FOM_BestBIC <- predict(FOM_BestBIC, newdata = valid.data.FO)

# Undo the previous transformations
Real_pred_FOM_BestBIC <- pred_FOM_BestBIC^2 - 1
Real_pred_FOM_BestBIC

# Find the error of the validation data
accuracy(Real_pred_FOM_BestBIC, valid.data.FO$Future_Goals)

# Best AIC Model
FOM_BestAIC <- lm(sqrt(Future_Goals) ~ Goals + First_Assists + iSCF + 
                    Takeaways + On_The_Fly_Starts + Off_Zone_Starts + 
                    Neu_Zone_Starts,data=train.data.FO)
# Summary
summary(FOM_BestAIC)

# Compute VIF
vif(FOM_BestAIC)

# Calculate the error
accuracy(FOM_BestAIC)

# Out-of-sample Prediction
pred_FOM_BestAIC <- predict(FOM_BestAIC, newdata = valid.data.FO)

# Undo the previous transformations
Real_pred_FOM_BestAIC <- pred_FOM_BestAIC^2 - 1
Real_pred_FOM_BestAIC

# Find the error of the validation data
accuracy(Real_pred_FOM_BestAIC, valid.data.FO$Future_Goals)
```

# Analysis:

I created five different regression models to predict the future goals for the forwards in the NHL. First, I separated the data into the training and validation sets with 70% of the data in the training set and 30% in the later. I then ran the full regression model with the variables being Goals, First_Assists, Second_Assists, IPP, ixG, iFF, iSCF, iHDCF, Rush_Attempts, Rebounds_Created, Takeaways, On_The_Fly_Starts, Off_Zone_Starts, Neu_Zone_Starts, and Def_Zone_Starts. This model had a solid adjusted r-square value but the residuals were not normally distributed. To fix this I ran the box cox transformation to find the ideal lambda and its corresponding transformation which happened to be the square root of the response variable. Next I re-ran the full model with the transformation and got a slightly worse adjusted r-square value but the residuals were fixed. I then ran the best subset model with the transformation to find the Cp, BIC, and AIC models. The Cp model and the AIC model turned out to be the same with 7 predictors which were all significant. The BIC model had 5 predictors with them all significant. After finding the VIF values it can be seen that all three of the sub models did not have multicollinearity but the full models did have issues with it. The last step was to undo the transformation and subtract the extra goal before predicting the future values and their error. 


# Picking the model:

```{r, warning=FALSE}
# Error table
knitr::include_graphics("C:/Users/nickg/OneDrive/Desktop/Capstone/FOM_Error_Table.png")
```

# Analysis:

For the forward offensive model it was tough to chose between the AIC/Cp model and the BIC but I decided to chose the BIC model as the best for a few reasons. First, both the RMSE and the MAE were the lowest even if they were close. Second, I know the BIC model had the lower of the adjusted r-square values but it is close enough to not make a huge difference. The deciding factor for me was that the BIC model is the simplest of the three. The predictors used for this model are Goals, iSCF, Takeaways, On_The_Fly_Starts, and Off_Zone_Starts. Each of these variables makes sense in a logical sense. I expected the number of goals, scoring chances, takeaways, and offensive zone starts to lead to future goals. I find it interesting that as the number of on the fly starts increases we expect the number of future goals to decrease. I feel this model does a good job predicting future goals without having extra variables like the AIC or Cp. This makes a difference when explaining and using the model in a practical setting like a hockey teams front office. 


# Preparing the data for the defense offensive model:

```{r, warning=FALSE, include=FALSE}
# Defense Offensive Model
# Combine the five seasons worth of data
hockey_data2 <- rbind(clean_data13_14, clean_data14_15, 
                      clean_data15_16,clean_data16_17, clean_data17_18)

head(hockey_data2)

# Rename the Goals variables
hockey_data2 <- rename(hockey_data2, Goals = Goals.x)
hockey_data2 <- rename(hockey_data2, Future_Goals = Goals.y)
hockey_data2 <- rename(hockey_data2, First_Assists = `First Assists`)
hockey_data2 <- rename(hockey_data2, Second_Assists = `Second Assists`)
hockey_data2 <- rename(hockey_data2, Rush_Attempts = `Rush Attempts`)
hockey_data2 <- rename(hockey_data2, Rebounds_Created = `Rebounds Created`)
hockey_data2 <- rename(hockey_data2, Off_Zone_Starts = `Off. Zone Starts`)
hockey_data2 <- rename(hockey_data2, Neu_Zone_Starts = `Neu. Zone Starts`)
hockey_data2 <- rename(hockey_data2, Def_Zone_Starts = `Def. Zone Starts`)
hockey_data2 <- rename(hockey_data2, On_The_Fly_Starts = `On The Fly Starts`)

# Find the missing values by column
Missing_values2 <- sort(colSums(is.na(hockey_data2)), decreasing=TRUE)
Missing_values2 <- Missing_values2[Missing_values2 > 0]
print(sort(Missing_values2), decreasing=TRUE)

# Prepare the data for the forwards 5v5 offense model
Defense_Offensive <- hockey_data2 %>%
  filter(Position == "D") %>%
  dplyr::select(Player:Rebounds_Created, Takeaways, On_The_Fly_Starts, 
                Off_Zone_Starts:Def_Zone_Starts, Future_Goals)

# Insert zero for the missing IPP values
Defense_Offensive$IPP[is.na(Defense_Offensive$IPP)] = 0

# Find the missing values by column
Missing_values <- sort(colSums(is.na(Defense_Offensive)), decreasing=TRUE)
Missing_values <- Missing_values[Missing_values > 0]
print(sort(Missing_values), decreasing=TRUE)

# Find the dimensions of the data
dim(Defense_Offensive)

# Filtering for games played and teams played
Defense_Offensive <- hockey_data %>%
  filter(Position == "D", GP > 10) %>%
  filter(str_detect(Team, pattern=",", negate=TRUE)) %>%
  dplyr::select(Player:Rebounds_Created, Takeaways, On_The_Fly_Starts, 
                Off_Zone_Starts:Def_Zone_Starts, Future_Goals)

# Find the dimensions of the data after filtering by GP and Team
dim(Defense_Offensive)
head(Defense_Offensive)

# Add one goal to the future goals 
Defense_Offensive <- Defense_Offensive %>%
  mutate(Future_Goals = Future_Goals + 1)

# Drop unwanted columns
Defense_Offensive <- Defense_Offensive[, -c(7,10,12,13,15)]

head(Defense_Offensive)
```

# Analysis:

The same process was used in the preprocessing for the defense offensive model as the forward offensive model with the exception that I filtered for defensemen instead of forwards. In this data set there are 1047 observations. Refer to the original steps for the process if needed. 


# Creating the models to predict defensmen's offense:

```{r, warning=FALSE}
# Regression analysis 
# Splitting data into training and validation sets
set.seed(1)
train.rows.DO <- sample(rownames(Defense_Offensive), nrow(Defense_Offensive) * 0.7)
train.data.DO <- Defense_Offensive[train.rows.DO, ]
valid.rows.DO <- setdiff(rownames(Defense_Offensive), train.rows.DO)
valid.data.DO <- Defense_Offensive[valid.rows.DO, ]

# Create a MLR model that contains the full desired predictors 
DOM_Full <- lm(Future_Goals ~ Goals + First_Assists + Second_Assists 
               + IPP + ixG + iFF + iSCF + iHDCF + Rush_Attempts 
               + Rebounds_Created + Takeaways + On_The_Fly_Starts + 
                 Off_Zone_Starts + Neu_Zone_Starts
               + Def_Zone_Starts, data = train.data.DO)
# Summary
summary(DOM_Full) 

# Get list of residuals 
Res_DOM_Full <- resid(DOM_Full)

# Produce residual vs. fitted plot
plot(fitted(DOM_Full), Res_DOM_Full)

# Add a horizontal line at 0 
abline(0,0)

# Create Q-Q plot for residuals
qqnorm(Res_DOM_Full)

# Add a straight diagonal line to the plot
qqline(Res_DOM_Full) 

# Create density plot of residuals
plot(density(Res_DOM_Full))

# Calculate the error
accuracy(DOM_Full)

# Out-of-sample Prediction
pred_DOM_Full <- predict(DOM_Full, newdata=valid.data.DO)
pred_DOM_Full

# Find the error of the validation data
accuracy(pred_DOM_Full, valid.data.DO$Future_Goals)

# Applying Box-Cox transformation to lm object
bc.results=boxcox(DOM_Full) #Applying Box-Cox transformation to lm object
lambda=bc.results$x[which.max(bc.results$y)] #extract the best lambda
lambda

lambda_DOM_Full <-lm(log(Future_Goals) ~ Goals + First_Assists + 
                       Second_Assists + IPP + ixG + iFF + iSCF + iHDCF 
                     + Rush_Attempts + Rebounds_Created + Takeaways + 
                       On_The_Fly_Starts + Off_Zone_Starts +  
                       Neu_Zone_Starts + Def_Zone_Starts, 
                       data=train.data.DO)

# Summary
summary(lambda_DOM_Full) 

# Get list of residuals 
lambda_Res_DOM_Full <- resid(lambda_DOM_Full)

# Produce residual vs. fitted plot
plot(fitted(lambda_DOM_Full), lambda_Res_DOM_Full)

# Add a horizontal line at 0 
abline(0,0)

# Create Q-Q plot for residuals
qqnorm(lambda_Res_DOM_Full)

# Add a straight diagonal line to the plot
qqline(lambda_Res_DOM_Full) 

# Create density plot of residuals
plot(density(lambda_Res_DOM_Full))

# Compute VIF
vif(DOM_Full)

# Compute VIF
vif(lambda_DOM_Full)

# Calculate the error
accuracy(lambda_DOM_Full)

# Out-of-sample Prediction
pred_lambda_DOM_Full <- predict(lambda_DOM_Full, newdata=valid.data.DO)

# Undo the previous transformations
MSE_Lambda_DOM_Full <- 0.5873^2
Real_pred_lambda_DOM_Full <- exp(pred_lambda_DOM_Full + 1/2*MSE_Lambda_DOM_Full) - 1
Real_pred_lambda_DOM_Full

# Find the error of the validation data
accuracy(Real_pred_lambda_DOM_Full, valid.data.DO$Future_Goals)

# Best subset selection
best_subset_DOM=regsubsets(log(Future_Goals) ~ Goals + First_Assists + 
                             Second_Assists + IPP + ixG + iFF + iSCF + 
                             iHDCF + Rush_Attempts + Rebounds_Created +
                             Takeaways + On_The_Fly_Starts + 
                             Off_Zone_Starts + Neu_Zone_Starts +
                          Def_Zone_Starts, data=train.data.DO,nvmax=15)

# Summarize the result
best_summary_DOM=summary(best_subset_DOM)
best_summary_DOM 

# Find the best Cp
p=2:16
Cp=cbind(p,best_summary_DOM$cp) #Pair p and Cp
colnames(Cp)=c('p','Cp') #Define column names 
print(Cp)

# Find BIC
p=2:16
BIC=cbind(p,best_summary_DOM$bic) #Pair p and BIC
colnames(BIC)=c('p','BIC') #Define column names 
print(BIC) 
plot(BIC,cex=2,pch=19) # Plot of BIC

# Find AIC
n=nrow(train.data.DO)
p=BIC[,1]
aic=BIC[,2]-p*log(n)+2*p #Need to compute AIC using BIC
AIC=cbind(2:16,aic) #Pair p and AIC
colnames(AIC)=c('p','AIC') #Define column names 
print(AIC) 
plot(AIC,cex=2,pch=19) #Plot of AIC


# Best Cp Model
DOM_BestCp <- lm(log(Future_Goals) ~ Goals + First_Assists + IPP + ixG 
                 + iSCF + iHDCF + Rush_Attempts + Rebounds_Created + 
                   On_The_Fly_Starts + Off_Zone_Starts + 
                  Neu_Zone_Starts + Def_Zone_Starts,data=train.data.DO)

# Summary
summary(DOM_BestCp)

# Calculate the error
accuracy(DOM_BestCp)

# Out-of-sample Prediction
pred_DOM_BestCp <- predict(DOM_BestCp, newdata=valid.data.DO)

# Undo the previous transformations
MSE_DOM_BestCp <- 0.5861^2
Real_pred_DOM_BestCp <- exp(pred_DOM_BestCp + 1/2*MSE_DOM_BestCp) - 1
Real_pred_DOM_BestCp

# Find the error of the validation data
accuracy(Real_pred_DOM_BestCp, valid.data.DO$Future_Goals)

# Compute VIF
vif(DOM_BestCp)

# Best BIC Model
DOM_BestBIC <- lm(log(Future_Goals) ~ ixG + iHDCF, data=train.data.DO)

# Summary
summary(DOM_BestBIC)

# Compute VIF
vif(DOM_BestBIC)

# Calculate the error
accuracy(DOM_BestBIC)

# Out-of-sample Prediction
pred_DOM_BestBIC <- predict(DOM_BestBIC, newdata=valid.data.DO)

# Undo the previous transformations
MSE_DOM_BestBIC <- 0.5863^2
Real_pred_DOM_BestBIC <- exp(pred_DOM_BestBIC + 1/2*MSE_DOM_BestBIC) - 1
Real_pred_DOM_BestBIC

# Find the error of the validation data
accuracy(Real_pred_DOM_BestBIC, valid.data.DO$Future_Goals)

# Best AIC Model
DOM_BestAIC <- lm(log(Future_Goals) ~ ixG + iHDCF + On_The_Fly_Starts + Neu_Zone_Starts, data=train.data.DO)

# Summary
summary(DOM_BestAIC)

# Compute VIF
vif(DOM_BestAIC)

# Calculate the error
accuracy(DOM_BestAIC)

# Out-of-sample Prediction
pred_DOM_BestAIC <- predict(DOM_BestAIC, newdata=valid.data.DO)

# Undo the previous transformations
MSE_DOM_BestAIC <- 0.5842^2
Real_pred_DOM_BestAIC <- exp(pred_DOM_BestAIC + 1/2*MSE_DOM_BestAIC) - 1
Real_pred_DOM_BestAIC

# Find the error of the validation data
accuracy(Real_pred_DOM_BestAIC, valid.data.DO$Future_Goals)
```

# Analysis:

Like the forward offensive model I created five different regression models to predict the future goals for the defensemen in the NHL. First, I separated the data into the training and validation sets with 70% of the data in the training set and 30% in the later. I then ran the full regression model with the same variables as before since I am still looking to predict players goals. These variables are Goals, First_Assists, Second_Assists, IPP, ixG, iFF, iSCF, iHDCF, Rush_Attempts, Rebounds_Created, Takeaways, On_The_Fly_Starts, Off_Zone_Starts, Neu_Zone_Starts, and Def_Zone_Starts. This model had a worse adjusted r-square value then the forward model but that can be expected since the model is for defensemen. Defensemen are known more for setting up goals then scoring them. Furthermore, the defensemen that do score a lot tend to do it on the power play not 5 on 5 meaning this model might not capture the full picture. like before the residuals were not normally distributed. To fix this I ran the box cox transformation to find the ideal lambda and its corresponding transformation which happened to be the log of the response variable. Next I re-ran the full model with the transformation and got a slightly worse adjusted r-square value but the residuals were much better. I then ran the best subset model with the transformation to find the Cp, BIC, and AIC models. After looking at the VIF values the Cp model and the full models had multicollinearity but the BIC and AIC models did not. The BIC model had 2 predictors and the AIC model had 4 predictors with them all being significant. The last part was undoing the log transformation and subtracting the goal that was added before predicting the future goals for defensemen. One value being observation 24 seemed to be extremely high for the data. I decided to keep the value since the error is really high and the player did score a high amount of goals the next season. 

# Picking the model:

```{r, warning=FALSE}
# Error table
knitr::include_graphics("C:/Users/nickg/OneDrive/Desktop/Capstone/DOM_Error_Table.png")
```

# Analysis:

I choose the AIC model as the best model because it has a higher adjusted r-square value than the BIC model and has a lower RMSE and MAE. The hardest part for me in this decision was the fact that the BIC model only has 2 predictors while the AIC model has 4. I ended up going with the more complex model because it is better across the board. Furthermore, the AIC models predictors are the same as the BIC but with On_The_Fly_Starts and Neu_Zone_Starts added. I think it makes perfect sense that the model included ixG otherwise known as individual expected goals. This is a stat that measures the probability of a shot being a goal based on location and angle. I found it interesting that iHDCF or individual high danger chances was negative. I thought that having more dangerous chances would lead to more future goals. This is something I want to look into further in the future.


# Create the response variable column:

```{r, warning=FALSE, include=FALSE}
# Defensive performance at 5v5
# Combine statistics with the next years actual goals against
# Find the goals of 2014_15
Goals_Against_14_15 <- Data_14_15 %>%
  dplyr::select(Player, GA)

head(Goals_Against_14_15)
dim(Goals_Against_14_15)

# Join goals14_15 with the 2013-14 data
clean_data13_14_Def <- inner_join(Data_13_14,
                              Goals_Against_14_15,
                              by="Player")
head(clean_data13_14_Def)

# Find the goals of 2015_16
Goals_Against_15_16 <- Data_15_16 %>%
  dplyr::select(Player, GA)

# Join goals15_16 with the 2014-15 data
clean_data14_15_Def <- inner_join(Data_14_15,
                              Goals_Against_15_16,
                              by="Player")

# Find the goals of 2016_17
Goals_Against_16_17 <- Data_16_17 %>%
  dplyr::select(Player, GA)

# Join goals16_17 with the 2015-16 data
clean_data15_16_Def <- inner_join(Data_15_16,
                              Goals_Against_16_17,
                              by="Player")

# Find the goals of 2017_18
Goals_Against_17_18 <- Data_17_18 %>%
  dplyr::select(Player, GA)

# Join goals17_18 with the 2016-17 data
clean_data16_17_Def <- inner_join(Data_16_17,
                              Goals_Against_17_18,
                              by="Player")

# Find the goals of 2018_19
Goals_Against_18_19 <- Data_18_19 %>%
  dplyr::select(Player, GA)

# Join goals18_19 with the 2017-18 data
clean_data17_18_Def <- inner_join(Data_17_18,
                              Goals_Against_18_19,
                              by="Player")

# Combine the five seasons worth of data
hockey_data3 <- rbind(clean_data13_14_Def, clean_data14_15_Def,   
                      clean_data15_16_Def, clean_data16_17_Def, 
                      clean_data17_18_Def)

head(hockey_data3)
```

# Analysis:

The preprocessing for the defensive models is almost the exact same as it was for the offensive models. The only difference is that for the defensive models the data for each year was joined with the amount of goals against that occurred while the player was on the ice the following season. This will be our response variable.


# Preparing the data for the forward defensive model:

```{r, warning=FALSE}
# Rename the Goals variables
hockey_data3 <- rename(hockey_data3, Goals_Against = GA.x)
hockey_data3 <- rename(hockey_data3, Future_Goals_Against = GA.y)
hockey_data3 <- rename(hockey_data3, First_Assists = `First Assists`)
hockey_data3 <- rename(hockey_data3, Second_Assists = `Second Assists`)
hockey_data3 <- rename(hockey_data3, Rush_Attempts = `Rush Attempts`)
hockey_data3 <- rename(hockey_data3, Rebounds_Created = `Rebounds Created`)
hockey_data3 <- rename(hockey_data3, Off_Zone_Starts = `Off. Zone Starts`)
hockey_data3 <- rename(hockey_data3, Neu_Zone_Starts = `Neu. Zone Starts`)
hockey_data3 <- rename(hockey_data3, Def_Zone_Starts = `Def. Zone Starts`)
hockey_data3 <- rename(hockey_data3, On_The_Fly_Starts = `On The Fly Starts`)

# Find the missing values by column
Missing_values3 <- sort(colSums(is.na(hockey_data3)), decreasing=TRUE)
Missing_values3 <- Missing_values3[Missing_values3 > 0]
print(sort(Missing_values3), decreasing=TRUE)

# Prepare the data for the forwards 5v5 defensive model
Forward_Defensive <- hockey_data3 %>%
  filter(Position == "R"|Position == "L"|Position == "C") %>%
  dplyr::select(Player:TOI,Giveaways:Takeaways,CA,xGA,SCA,HDCA,HDGA,PDO
                ,On_The_Fly_Starts,Off_Zone_Starts:Def_Zone_Starts,
                Future_Goals_Against)

# Find the missing values by column
Missing_values3 <- sort(colSums(is.na(Forward_Defensive)), decreasing=TRUE)
Missing_values3 <- Missing_values3[Missing_values3 > 0]
print(sort(Missing_values3), decreasing=TRUE)

# Find the dimensions of the data
dim(Forward_Defensive)

# Filtering for games played and teams played
Forward_Defensive <- hockey_data3 %>%
  filter(Position == "R"|Position == "L"|Position == "C", GP > 10) %>%
  filter(str_detect(Team, pattern=",", negate=TRUE)) %>%
  dplyr::select(Player:TOI,Giveaways:Takeaways,CA,xGA,SCA,HDCA,HDGA,PDO
                ,On_The_Fly_Starts,Off_Zone_Starts:Def_Zone_Starts,
                Future_Goals_Against)

# Find the dimensions of the data after filtering by GP and Team
dim(Forward_Defensive)
head(Forward_Defensive)
```

# Analysis:

Before the defensive regression models could be made there needed to be some preprocessing. First, some variables were renamed to make them more convenient to work with. Second, It turns out that all of the variables that had missing values were not used in the analysis. Third, the data set was filtered to only included centers, left wings, and right wings, otherwise known as forwards. The full data set for forwards has the same number of observations as in the forward offensive model which includes 2291 observations. For this model there is 18 variables. In the subset of the data after filtering out players who played with more than one team in a season or played in less then 10 games, there is 1856 observations and 18 variables. After dropping unwanted variables the data set includes Player, Team, Position, GP, TOI, Giveaways, Takeaways, CA, xGA, SCA, HDCA, HDGA, PDO, On_The_Fly_Starts, Off_Zone_Starts, Neu_Zone_Starts, Def_Zone_Starts, and Future_Goals_Against. Unlike the offensive model there was no need to add one goal to the Future goals against column because no transformation was needed.


# Creating the models to predict forwards defense:

```{r, warning=FALSE}
# Regression analysis 
# Splitting data into training and validation sets
set.seed(1)
train.rows.FD <- sample(rownames(Forward_Defensive), nrow(Forward_Defensive) * 0.7)
train.data.FD <- Forward_Defensive[train.rows.FD, ]
valid.rows.FD <- setdiff(rownames(Forward_Defensive), train.rows.FD)
valid.data.FD <- Forward_Defensive[valid.rows.FD, ]

# Create a MLR model that contains the full desired predictors 
FDM_Full <- lm(Future_Goals_Against ~ Giveaways + CA + xGA + SCA + 
                 HDCA + On_The_Fly_Starts + Neu_Zone_Starts
               + Def_Zone_Starts, data=train.data.FD)
# Summary
summary(FDM_Full) 

# Get list of residuals 
Res_FDM_Full <- resid(FDM_Full)

# Produce residual vs. fitted plot
plot(fitted(FDM_Full), Res_FDM_Full)

# Add a horizontal line at 0 
abline(0,0)

# Create Q-Q plot for residuals
qqnorm(Res_FDM_Full)

# Add a straight diagonal line to the plot
qqline(Res_FDM_Full) 

# Create density plot of residuals
plot(density(Res_FDM_Full))

# Compute VIF
vif(FDM_Full)

# Calculate the error
accuracy(FDM_Full)

# Out-of-sample Prediction
pred_FDM_Full <- predict(FDM_Full, newdata=valid.data.FD)
pred_FDM_Full

# Find the error of the validation data
accuracy(pred_FDM_Full, valid.data.FD$Future_Goals_Against)

# Best subset selection
best_subset_FDM=regsubsets(Future_Goals_Against ~ Giveaways + CA + xGA 
                           + SCA + HDCA + On_The_Fly_Starts + 
                             Neu_Zone_Starts + Def_Zone_Starts, 
                             data=train.data.FD, nvmax=8)

# Summarize the result
best_summary_FDM=summary(best_subset_FDM)
best_summary_FDM 

# Find the best Cp
p=2:9
Cp=cbind(p,best_summary_FDM$cp) #Pair p and Cp
colnames(Cp)=c('p','Cp') #Define column names 
print(Cp)

# Find BIC
p=2:9
BIC=cbind(p,best_summary_FDM$bic) #Pair p and BIC
colnames(BIC)=c('p','BIC') #Define column names 
print(BIC) 
plot(BIC,cex=2,pch=19) # Plot of BIC

# Find AIC
n=nrow(train.data.FD)
p=BIC[,1]
aic=BIC[,2]-p*log(n)+2*p #Need to compute AIC using BIC
AIC=cbind(2:9,aic) #Pair p and AIC
colnames(AIC)=c('p','AIC') #Define column names 
print(AIC) 
plot(AIC,cex=2,pch=19) #Plot of AIC


# Best Cp Model
FDM_BestCp <- lm(Future_Goals_Against ~ Giveaways + CA + HDCA + 
                   On_The_Fly_Starts + Neu_Zone_Starts + 
                   Def_Zone_Starts,data=train.data.FD)
# Summary
summary(FDM_BestCp)

# Compute VIF
vif(FDM_BestCp)

# Calculate the error
accuracy(FDM_BestCp)

# Out-of-sample Prediction
pred_FDM_BestCp <- predict(FDM_BestCp, newdata = valid.data.FD)
pred_FDM_BestCp

# Find the error of the validation data
accuracy(pred_FDM_BestCp, valid.data.FD$Future_Goals_Against)

# Best BIC Model
FDM_BestBIC <- lm(Future_Goals_Against ~ Giveaways + xGA + 
                    On_The_Fly_Starts,data=train.data.FD)

# Summary
summary(FDM_BestBIC)

# Compute VIF
vif(FDM_BestBIC)

# Calculate the error
accuracy(FDM_BestBIC)

# Out-of-sample Prediction
pred_FDM_BestBIC <- predict(FDM_BestBIC, newdata=valid.data.FD)
pred_FDM_BestBIC

# Find the error of the validation data
accuracy(pred_FDM_BestBIC, valid.data.FD$Future_Goals_Against)

# Best AIC Model
FDM_BestAIC <- lm(Future_Goals_Against ~ Giveaways + CA + HDCA + 
                    On_The_Fly_Starts + Neu_Zone_Starts + 
                    Def_Zone_Starts, data=train.data.FD)

# Summary
summary(FDM_BestAIC)

# Compute VIF
vif(FDM_BestAIC)

# Calculate the error
accuracy(FDM_BestAIC)

# Out-of-sample Prediction
pred_FDM_BestAIC <- predict(FDM_BestAIC, newdata = valid.data.FD)
pred_FDM_BestAIC

# Find the error of the validation data
accuracy(pred_FDM_BestAIC, valid.data.FD$Future_Goals_Against)
```

# Analysis:

Like the forward models I created a range of different regression models to predict the future goals against for the forwards in the NHL. First, I separated the data into the training and validation sets with 70% of the data in the training set and 30% in the later. I then ran the full regression model with the variables Giveaways, CA, xGA, SCA, HDCA, On_The_Fly_Starts, Neu_Zone_Starts, and Def_Zone_Starts. This model had a worse adjusted r-square value then the forward offensive model but better than the forward defensive models. I found this surprising since there is so much that a player cannot control like his line mates, goalie, and the coaches decisions so I thought the model would be worse. Unlike before, the residuals were fine so no transformation was needed. I then ran the best subset model to find the Cp, BIC, and AIC models. After looking at the VIF values I found that the only model without multicollinearity being the BIC model. Because of this I chose the BIC model as my best model to predict forwards future goals against. The BIC model had 3 predictors and they are all significant. Since this model did not need a transformation there was no need to adjust the prediction and error calculation. 

# Picking the model:

```{r, warning=FALSE}
# Error table
knitr::include_graphics("C:/Users/nickg/OneDrive/Desktop/Capstone/FDM_Error_Table.png")
```

# Analysis:

I really did not have much of a choice when picking the model because the BIC model is the only one that does not have multicollinearity. This model has the highest RMSE and MAE but also has the smallest number of predictors. This is definitely not ideal but the good news is the variables in the model make sense from a logical perspective. There are only three predictors in this model which are giveaways, XGA otherwise known as the probability of an opponents shot becoming a goal, and on the fly starts. It makes sense that the number of giveaways and the better the quality chances would be good predictors of future goals against. As for the on the fly starts it appears to have the same effect as previous models where it is negatively correlated. Overall this model seems to be okay given the amount of uncertainty that comes with a players defense. 


# Preparing the data for the defense defensive model:

```{r, warning=FALSE, include=FALSE}
# Combine the five seasons worth of data
hockey_data4 <- rbind(clean_data13_14_Def, clean_data14_15_Def, 
                      clean_data15_16_Def, clean_data16_17_Def, 
                      clean_data17_18_Def)

head(hockey_data4)

# Rename the Goals variables
hockey_data4 <- rename(hockey_data4, Goals_Against = GA.x)
hockey_data4 <- rename(hockey_data4, Future_Goals_Against = GA.y)
hockey_data4 <- rename(hockey_data4, First_Assists = `First Assists`)
hockey_data4 <- rename(hockey_data4, Second_Assists = `Second Assists`)
hockey_data4 <- rename(hockey_data4, Rush_Attempts = `Rush Attempts`)
hockey_data4 <- rename(hockey_data4, Rebounds_Created = `Rebounds Created`)
hockey_data4 <- rename(hockey_data4, Off_Zone_Starts = `Off. Zone Starts`)
hockey_data4 <- rename(hockey_data4, Neu_Zone_Starts = `Neu. Zone Starts`)
hockey_data4 <- rename(hockey_data4, Def_Zone_Starts = `Def. Zone Starts`)
hockey_data4 <- rename(hockey_data4, On_The_Fly_Starts = `On The Fly Starts`)

# Find the missing values by column
Missing_values4 <- sort(colSums(is.na(hockey_data4)), decreasing=TRUE)
Missing_values4 <- Missing_values4[Missing_values4 > 0]
print(sort(Missing_values4), decreasing=TRUE)

# Prepare the data for the forwards 5v5 defensive model
Defense_Defensive <- hockey_data4 %>%
  filter(Position == "D") %>%
  dplyr::select(Player:TOI,Giveaways:Takeaways,CA,xGA,SCA,HDCA,HDGA,PDO
                ,On_The_Fly_Starts,Off_Zone_Starts:Def_Zone_Starts,
                Future_Goals_Against)

# Find the missing values by column
Missing_values4 <- sort(colSums(is.na(Defense_Defensive)), decreasing=TRUE)
Missing_values4 <- Missing_values4[Missing_values4 > 0]
print(sort(Missing_values4), decreasing=TRUE)

# Find the dimensions of the data
dim(Defense_Defensive)

# Filtering for games played and teams played
Defense_Defensive <- hockey_data4 %>%
  filter(Position == "D", GP > 10) %>%
  filter(str_detect(Team, pattern=",", negate=TRUE)) %>%
  dplyr::select(Player:TOI,Giveaways:Takeaways,CA,xGA,SCA,HDCA,HDGA,PDO
                ,On_The_Fly_Starts,Off_Zone_Starts:Def_Zone_Starts,
                Future_Goals_Against)

# Find the dimensions of the data after filtering by GP and Team
dim(Defense_Defensive)
head(Defense_Defensive)
```

# Analysis:

The preprocessing for the defensemen defensive model was almost the exact same as for the forward defensive model with the only difference being the data was filtered for defensemen. Like the other defensemen models there were 1047 observations.


# Creating the models to predict defensemen defense:

```{r, warning=FALSE}
# Regression analysis 
# Splitting data into training and validation sets
set.seed(1)
train.rows.DD <- sample(rownames(Defense_Defensive), nrow(Defense_Defensive) * 0.7)
train.data.DD <- Defense_Defensive[train.rows.DD, ]
valid.rows.DD <- setdiff(rownames(Defense_Defensive), train.rows.DD)
valid.data.DD <- Defense_Defensive[valid.rows.DD, ]

# Create a MLR model that contains the full desired predictors 
DDM_Full <- lm(Future_Goals_Against ~ Giveaways + CA + xGA + SCA + 
                 HDCA + On_The_Fly_Starts + Neu_Zone_Starts
               + Def_Zone_Starts, data=train.data.DD)
# Summary
summary(DDM_Full) 

# Get list of residuals 
Res_DDM_Full <- resid(DDM_Full)

# Produce residual vs. fitted plot
plot(fitted(DDM_Full), Res_DDM_Full)

# Add a horizontal line at 0 
abline(0,0)

# Create Q-Q plot for residuals
qqnorm(Res_DDM_Full)

# Add a straight diagonal line to the plot
qqline(Res_DDM_Full) 

# Create density plot of residuals
plot(density(Res_DDM_Full))

# Compute VIF
vif(DDM_Full)

# Calculate the error
accuracy(DDM_Full)

# Out-of-sample Prediction
pred_DDM_Full <- predict(DDM_Full, newdata=valid.data.DD)
pred_DDM_Full

# Find the error of the validation data
accuracy(pred_DDM_Full, valid.data.DD$Future_Goals_Against)

# Best subset selection
best_subset_DDM=regsubsets(Future_Goals_Against ~ Giveaways + CA + xGA 
                           + SCA + HDCA + On_The_Fly_Starts + 
                             Neu_Zone_Starts + Def_Zone_Starts, 
                           data=train.data.DD, nvmax=8)

# Summarize the result
best_summary_DDM=summary(best_subset_DDM)
best_summary_DDM 

# Find the best Cp
p=2:9
Cp=cbind(p,best_summary_DDM$cp) #Pair p and Cp
colnames(Cp)=c('p','Cp') #Define column names 
print(Cp)

# Find BIC
p=2:9
BIC=cbind(p,best_summary_DDM$bic) #Pair p and BIC
colnames(BIC)=c('p','BIC') #Define column names 
print(BIC) 
plot(BIC,cex=2,pch=19) # Plot of BIC

# Find AIC
n=nrow(train.data.DD)
p=BIC[,1]
aic=BIC[,2]-p*log(n)+2*p #Need to compute AIC using BIC
AIC=cbind(2:9,aic) #Pair p and AIC
colnames(AIC)=c('p','AIC') #Define column names 
print(AIC) 
plot(AIC,cex=2,pch=19) #Plot of AIC


# Best Cp Model
DDM_BestCp <- lm(Future_Goals_Against ~ Giveaways + CA + xGA + SCA + 
                On_The_Fly_Starts + Neu_Zone_Starts,data=train.data.DD)

# Summary
summary(DDM_BestCp)

# Compute VIF
vif(DDM_BestCp)

# Calculate the error
accuracy(DDM_BestCp)

# Out-of-sample Prediction
pred_DDM_BestCp <- predict(DDM_BestCp, newdata=valid.data.DD)
pred_DDM_BestCp

# Find the error of the validation data
accuracy(pred_DDM_BestCp, valid.data.DD$Future_Goals_Against)

# Best BIC Model
DDM_BestBIC <- lm(Future_Goals_Against ~ Giveaways + xGA + On_The_Fly_Starts + 
                  Neu_Zone_Starts,data=train.data.DD)

# Summary
summary(DDM_BestBIC)

# Compute VIF
vif(DDM_BestBIC)

# Calculate the error
accuracy(DDM_BestBIC)

# Out-of-sample Prediction
pred_DDM_BestBIC <- predict(DDM_BestBIC, newdata=valid.data.DD)
pred_DDM_BestBIC

# Find the error of the validation data
accuracy(pred_DDM_BestBIC, valid.data.DD$Future_Goals_Against)

# Best AIC Model
DDM_BestAIC <- lm(Future_Goals_Against ~ Giveaways + xGA + SCA + 
                    On_The_Fly_Starts + Neu_Zone_Starts, 
                    data=train.data.DD)

# Summary
summary(DDM_BestAIC)

# Compute VIF
vif(DDM_BestAIC)

# Calculate the error
accuracy(DDM_BestAIC)

# Out-of-sample Prediction
pred_DDM_BestAIC <- predict(DDM_BestAIC, newdata=valid.data.DD)
pred_DDM_BestAIC

# Find the error of the validation data
accuracy(pred_DDM_BestAIC, valid.data.DD$Future_Goals_Against)
```

# Analysis:

Like the other models for predicting future goals against I created a range of different regression models. First, I separated the data into the training and validation sets with 70% of the data in the training set and 30% in the later. I then ran the full regression model with the same variables as the forward defensive model. These variables are Giveaways, CA, xGA, SCA, HDCA, On_The_Fly_Starts, Neu_Zone_Starts, and Def_Zone_Starts. These models had a slightly better adjusted r-square value then the forward defensive models. I found this to be encouraging since it is the defensemen's number one job to stop the other team from scoring. Like the forward defensive models, there is so much that a player cannot control like his line mates, goalie, and the coaches decisions so I thought the model would not be as good at predicting future goals against. Like the forward defensive models, the residuals were fine so no transformation was needed. I then ran the best subset model to find the Cp, BIC, and AIC models. After looking at the VIF values I found that the only model without multicollinearity being the BIC model. For this reason I chose the BIC model as my best model to predict forwards future goals against. The BIC model had 4 predictors and they are all significant. Since this model did not need a transformation there was no need to adjust the prediction and error calculation. 

# Picking the model:

```{r, warning=FALSE}
# Error table
knitr::include_graphics("C:/Users/nickg/OneDrive/Desktop/Capstone/DDM_Error_Table.png")
```

# Analysis:

Like the forward defensive model I really did not have much of a choice when picking the model because the BIC model is the only one that does not have multicollinearity. This model has the lowest RMSE and the second lowest MAE but also has the smallest number of predictors. It is definitely not ideal that the error is so high but the good news is most of the variables in the model make sense from a logical perspective. There are only four predictors in this model which are giveaways, XGA otherwise known as the probability of an opponents shot becoming a goal, on the fly starts, and neutral zone starts. It makes sense that the number of giveaways and the better the quality chances would be good predictors of future goals against. As for the on the fly starts it appears to have the same effect as previous models where it is negatively correlated. The one variable that does not make sense is the neutral zone starts. I would have thought that having defensive zone starts would make more sense then neutral zone starts because that gives the opponent a better chance to score off the faceoff. One way to explain this is that coaches like to put there best defensive players on the ice for defensive zone starts which could lead to the team clearing the zone thus not getting scored on. Overall this model seems to be okay given the amount of uncertainty that comes with a players defense. 

# Conclusion:

In conclusion I think the models created did a relatively good job at predicting a players future goals and goals against. I think in the future more models should be developed to further study this topic. I could possibly use ridge regression or lasso regression to deal with the multicollinearity. Furthermore, I could try using random forests or neural nets to get a better model. Another idea to improve the model is to use rates. I did try this idea briefly but the model had a really small adjusted r-square value so I abandon the idea. Maybe with more time I could better understand why this idea did not work. In the future I would like to add more models for different phases of the game such as power play and penalty kill. The last idea to improve the model is to find better data. Overall, I am proud of the project I created but I definitely want to improve it in the future. 



